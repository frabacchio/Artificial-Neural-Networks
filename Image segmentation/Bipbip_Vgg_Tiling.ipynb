{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Bipbip_Vgg_Tiling.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4m1NWHVJmje","executionInfo":{"status":"ok","timestamp":1607957118713,"user_tz":-60,"elapsed":25273,"user":{"displayName":"Edoardo Francesco Mameo","photoUrl":"","userId":"14965487874433148146"}},"outputId":"8d498a02-189a-4e69-a175-47be879d278d"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","import os\n","import tensorflow as tf\n","import numpy as np\n","\n","# Set the seed for random operations. \n","# This let our experiments to be reproducible. \n","SEED = 1234\n","tf.random.set_seed(SEED)  \n","\n","cwd = os.getcwd()\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sFHrtOWRJxDP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607957217698,"user_tz":-60,"elapsed":79608,"user":{"displayName":"Edoardo Francesco Mameo","photoUrl":"","userId":"14965487874433148146"}},"outputId":"887aa9ea-373c-49f5-b4c1-aa8ffe0c7be9"},"source":["# ImageDataGenerator\n","# ------------------\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.vgg16 import preprocess_input \n","apply_data_augmentation = False\n","\n","# Create training ImageDataGenerator object\n","# We need two different generators for images and corresponding masks\n","if apply_data_augmentation:\n","    train_img_data_gen = ImageDataGenerator(rotation_range=10,\n","                                      width_shift_range=10,\n","                                      height_shift_range=10,\n","                                      zoom_range=0.3,\n","                                      horizontal_flip=True,\n","                                      vertical_flip=True,\n","                                      fill_mode='reflect',\n","                                      preprocessing_function=preprocess_input,\n","                                      validation_split=0.2)\n","    train_mask_data_gen = ImageDataGenerator(rotation_range=10,\n","                                      width_shift_range=10,\n","                                      height_shift_range=10,\n","                                      zoom_range=0.3,\n","                                      horizontal_flip=True,\n","                                      vertical_flip=True,\n","                                      fill_mode='reflect',\n","                                      validation_split=0.2)\n","else:\n","    train_img_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input,validation_split=0.2)\n","    train_mask_data_gen = ImageDataGenerator(validation_split=0.2)\n","\n","# Create validation ImageDataGenerator objects\n","valid_img_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input,validation_split=0.2)\n","valid_mask_data_gen = ImageDataGenerator(validation_split=0.2)\n","\n","# Create generators to read images from dataset directory\n","# -------------------------------------------------------\n","\n","dataset_dir = '/content/drive/My Drive/AN2DL/Challenge2/Bip/Training256'\n","# Batch size\n","bs = 8\n","\n","# img shape\n","img_h = 256\n","img_w = 256\n","num_classes=3\n","\n","# Training\n","# Two different generators for images and masks but with same seed\n","training_dir = os.path.join(dataset_dir, 'Mais') #here put 'Haricot' to train on haricot dataset\n","train_img_gen = train_img_data_gen.flow_from_directory(os.path.join(training_dir, 'Images'),\n","                                                      target_size=(img_h, img_w),\n","                                                      batch_size=bs,\n","                                                      class_mode=None, \n","                                                      shuffle=True,\n","                                                      interpolation='bilinear',\n","                                                      seed=SEED,\n","                                                      subset='training')  \n","train_mask_gen = train_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'Masks'),\n","                                                        target_size=(img_h, img_w),\n","                                                        batch_size=bs,\n","                                                        class_mode=None,\n","                                                        shuffle=True,\n","                                                        interpolation='nearest',\n","                                                        seed=SEED,\n","                                                        subset='training')\n","train_gen = zip(train_img_gen, train_mask_gen)\n","\n","# Validation\n","valid_img_gen = valid_img_data_gen.flow_from_directory(os.path.join(training_dir, 'Images'),\n","                                                      target_size=(img_h, img_w),\n","                                                      batch_size=bs, \n","                                                      class_mode=None, \n","                                                      shuffle=True,\n","                                                      interpolation='bilinear',\n","                                                      seed=SEED,\n","                                                      subset='validation')\n","valid_mask_gen = valid_mask_data_gen.flow_from_directory(os.path.join(training_dir, 'Masks'),\n","                                                        target_size=(img_h, img_w),\n","                                                        batch_size=bs, \n","                                                        class_mode=None,\n","                                                        shuffle=True,\n","                                                        interpolation='nearest',\n","                                                        seed=SEED,\n","                                                        subset='validation')\n","valid_gen = zip(valid_img_gen, valid_mask_gen)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 3456 images belonging to 1 classes.\n","Found 3456 images belonging to 1 classes.\n","Found 864 images belonging to 1 classes.\n","Found 864 images belonging to 1 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u7xm26qGKPeP"},"source":["train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None,img_h, img_w, 3], [None,img_h, img_w,3]))\n","\n","\n","def prepare_target(x_, y_):\n","  # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target labels 256 x 256 x 1\n","  #[254, 124, 18] = 0 done implicitly with the first cast\n","  #[255, 255, 255]= 1 done with the first cast\n","  #[216, 67, 82]= 2 done with the second cast that is multiplied by 2\n","    output=tf.cast(tf.reduce_all(y_== [255, 255, 255], axis=-1, keepdims=True), tf.float32)\n","    output=output+2*tf.cast(tf.reduce_all(y_== [216, 67, 82], axis=-1, keepdims=True), tf.float32)\n","    return x_, output\n","\n","train_dataset = train_dataset.map(prepare_target)\n","\n","\n","train_dataset = train_dataset.repeat()\n","\n","\n","\n","valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen,\n","                                               output_types=(tf.float32, tf.float32),\n","                                               output_shapes=([None,img_h, img_w, 3], [None,img_h, img_w,3]))\n","valid_dataset = valid_dataset.map(prepare_target)\n","\n","\n","valid_dataset = valid_dataset.repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0zkEISV173q"},"source":["# Create Model using VGG16 as encoder\n","# ------------\n","vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(None, None, 3))\n","\n","finetuning = True\n","\n","if finetuning:\n","    freeze_until = 17 # layer from which we want to fine-tune\n","    \n","    for layer in vgg.layers[:freeze_until]:\n","        layer.trainable = False\n","else:\n","    vgg.trainable = False\n","\n","\n","def create_model(depth, num_classes):\n","\n","    model = tf.keras.Sequential()\n","    \n","    # Encoder\n","    # -------\n","    model.add(vgg)\n","    \n","    start_f = 256\n","        \n","    # Decoder\n","    # -------\n","    for i in range(depth):\n","        model.add(tf.keras.layers.UpSampling2D(2, interpolation='bilinear'))\n","        model.add(tf.keras.layers.Conv2D(filters=start_f,\n","                                         kernel_size=(3, 3),\n","                                         strides=(1, 1),\n","                                         padding='same'))\n","        model.add(tf.keras.layers.ReLU())\n","\n","        start_f = start_f // 2\n","\n","    # Prediction Layer\n","    # ----------------\n","    model.add(tf.keras.layers.Conv2D(filters=num_classes,\n","                                     kernel_size=(1, 1),\n","                                     strides=(1, 1),\n","                                     padding='same',\n","                                     activation='softmax'))\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Csp56nU0lx85","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607954948089,"user_tz":-60,"elapsed":991,"user":{"displayName":"Fabio Menozzi","photoUrl":"","userId":"17453204346991109173"}},"outputId":"12ab5f37-0fcc-4d2b-a4b7-1ea97dce6287"},"source":["model = create_model(depth=5, \n","                     num_classes=3)\n","\n","# Visualize created model as a table\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, None, None, 512)   14714688  \n","_________________________________________________________________\n","up_sampling2d (UpSampling2D) (None, None, None, 512)   0         \n","_________________________________________________________________\n","conv2d (Conv2D)              (None, None, None, 256)   1179904   \n","_________________________________________________________________\n","re_lu (ReLU)                 (None, None, None, 256)   0         \n","_________________________________________________________________\n","up_sampling2d_1 (UpSampling2 (None, None, None, 256)   0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, None, None, 128)   295040    \n","_________________________________________________________________\n","re_lu_1 (ReLU)               (None, None, None, 128)   0         \n","_________________________________________________________________\n","up_sampling2d_2 (UpSampling2 (None, None, None, 128)   0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, None, None, 64)    73792     \n","_________________________________________________________________\n","re_lu_2 (ReLU)               (None, None, None, 64)    0         \n","_________________________________________________________________\n","up_sampling2d_3 (UpSampling2 (None, None, None, 64)    0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, None, None, 32)    18464     \n","_________________________________________________________________\n","re_lu_3 (ReLU)               (None, None, None, 32)    0         \n","_________________________________________________________________\n","up_sampling2d_4 (UpSampling2 (None, None, None, 32)    0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, None, None, 16)    4624      \n","_________________________________________________________________\n","re_lu_4 (ReLU)               (None, None, None, 16)    0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, None, None, 3)     51        \n","=================================================================\n","Total params: 16,286,563\n","Trainable params: 3,931,683\n","Non-trainable params: 12,354,880\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mka78Y4Fcp37"},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","# Sparse Categorical Crossentropy to use integers (mask) instead of one-hot encoded labels\n","loss = tf.keras.losses.SparseCategoricalCrossentropy() \n","# learning rate\n","lr = 1e-3\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Here we define the intersection over union for each class in the batch.\n","# Then we compute the final iou as the mean over classes\n","def meanIoU(y_true, y_pred):\n","    # get predicted class from softmax\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(1,3): # exclude the background class 0\n","      # Get prediction and target related to only a single class (i)\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-7) / (union + 1e-7)\n","      per_class_iou.append(iou)\n","\n","    return tf.reduce_mean(per_class_iou)\n","\n","# Validation metrics\n","# ------------------\n","metrics = ['accuracy', meanIoU]\n","# ------------------\n","\n","# Compile Model\n","model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSLexxFwcuXz"},"source":["#training with callbacks\n","callbacks = []\n","\n","# Early Stopping\n","# --------------\n","early_stop = True\n","if early_stop:\n","    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n","    callbacks.append(es_callback)\n","# Learning rate adapter\n","# --------------\n","lr_adapter = True\n","if lr_adapter:\n","    lr_adapter_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, verbose=1, mode='auto', min_delta=0.0001, cooldown=0,restore_best_weights=True)\n","    callbacks.append(lr_adapter_callback)\n","    \n","#training\n","model.fit(x=train_dataset,\n","          epochs=50,\n","          steps_per_epoch=len(train_img_gen ),\n","          validation_data=valid_dataset,\n","          validation_steps=len(valid_img_gen ), \n","           callbacks=callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0pu5D5RP-r-i"},"source":["#save model for final prediction\n","model.save('/content/drive/My Drive/AN2DL/Challenge2/Bip/SavedModel/Mais256_vgg.h5')\n","\n","#model.save('/content/drive/My Drive/AN2DL/Challenge2/Bip/SavedModel/Haricot256_vgg.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zvelv8bUPVlt"},"source":["#load the two model for predictions\n","#one model will predict all haricot datasets\n","#the other one is used for mais datasets\n","\n","def meanIoU(y_true, y_pred):\n","    # get predicted class from softmax\n","    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n","\n","    per_class_iou = []\n","\n","    for i in range(1,3): # exclude the background class 0\n","      # Get prediction and target related to only a single class (i)\n","      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n","      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n","      intersection = tf.reduce_sum(class_true * class_pred)\n","      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n","    \n","      iou = (intersection + 1e-7) / (union + 1e-7)\n","      per_class_iou.append(iou)\n","\n","    return tf.reduce_mean(per_class_iou)\n","\n","model_haricot=tf.keras.models.load_model('/content/drive/My Drive/AN2DL/Challenge2/Bip/SavedModel/Haricot256_vgg.h5',custom_objects={'meanIoU':meanIoU})\n","\n","model_mais=tf.keras.models.load_model('/content/drive/My Drive/AN2DL/Challenge2/Bip/SavedModel/Mais256_vgg.h5',custom_objects={'meanIoU':meanIoU})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NzyV2uU8kcO"},"source":["#prepare submission \n","\n","from PIL import Image\n","def rle_encode(img):\n","    '''\n","    img: numpy array, 1 - foreground, 0 - background\n","    Returns run length as string formatted\n","    '''\n","    pixels = img.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)\n","\n","submission_dict = {}\n","groups=['Bipbip','Pead','Roseau','Weedelec']\n","hm=['Haricot','Mais']\n","for g in groups:\n","  for h in hm:\n","      final_path = '/content/drive/My Drive/AN2DL/Challenge2/Development_Dataset/Test_Dev/'+ str(g) + '/'+ str(h) +'/Images'\n","      image_filenames = next(os.walk(final_path))[2]   \n","      for img_name in image_filenames:\n","          img=Image.open(final_path+'/'+img_name).convert('RGB')\n","          img_width,img_heigth=img.size\n","          #vgg preprocessing\n","          img_array=np.array(img)\n","          img_array=preprocess_input(img_array)\n","          height = 256\n","          width = 256\n","          #creation of patches of subimages with size 256*256*3\n","          subimages=tf.image.extract_patches(images=tf.expand_dims(tf.convert_to_tensor(img_array), 0),\n","                           sizes=[1, width, height, 1],\n","                           strides=[1, width, height, 1],\n","                           rates=[1, 1, 1, 1],\n","                           padding='SAME') #pad with zeros in case of missmatching dimenzion\n","          \n","\n","          number_subimages=subimages.shape[1]*subimages.shape[2]\n","          n_row=subimages.shape[1]\n","          n_col=subimages.shape[2]\n","          subimages=tf.reshape(subimages,[number_subimages,256,256,3])\n","          #we compute segmentation for each subimage and then we reconstrcut the final mask\n","          for i in range(0, n_row):\n","            for j in range(0,n_col):\n","              if h=='Haricot':\n","                  out_sigmoid = model_haricot.predict(x=tf.expand_dims(subimages[i*n_col+j,:,:,:], 0))\n","              else:\n","                  out_sigmoid = model_mais.predict(x=tf.expand_dims(subimages[i*n_col+j,:,:,:], 0))\n","\n","              predicted_class = tf.argmax(out_sigmoid, -1)\n","              predicted_class = predicted_class[0, ...]\n","              mask_arr = np.array(predicted_class)\n","              if j==0:\n","                      mask_arr_row=mask_arr\n","              else:\n","                      mask_arr_row=np.hstack((mask_arr_row,mask_arr))\n","            if i==0:\n","                mask_arr_final=mask_arr_row\n","            else:\n","                mask_arr_final=np.vstack((mask_arr_final,mask_arr_row))\n","\n","          mask_arr_final=mask_arr_final[:img_heigth,:img_width]   \n","          # RLE encoding\n","          # crop\n","          rle_encoded_crop = rle_encode(mask_arr_final == 1)\n","          # weed\n","          rle_encoded_weed = rle_encode(mask_arr_final == 2)\n","          img_name_without_extension=img_name[:-4]\n","          submission_dict[img_name_without_extension] = {}\n","          submission_dict[img_name_without_extension]['shape'] =mask_arr_final.shape\n","          submission_dict[img_name_without_extension]['team'] = g\n","          submission_dict[img_name_without_extension]['crop'] = h\n","          submission_dict[img_name_without_extension]['segmentation'] = {}\n","          submission_dict[img_name_without_extension]['segmentation']['crop'] = rle_encoded_crop\n","          submission_dict[img_name_without_extension]['segmentation']['weed'] = rle_encoded_weed\n"," \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TNWK2tMx8mW_"},"source":["#create json file for submission\n","import json\n","with open('/content/drive/My Drive/AN2DL/Challenge2submission.json', 'w') as f:\n","  json.dump(submission_dict, f)"],"execution_count":null,"outputs":[]}]}